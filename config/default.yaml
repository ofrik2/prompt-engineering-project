# Default configuration for prompt-engineering experiments

model:
  provider: "dummy"
  model_name: "primary"   # or "secondary" or a specific deployment name as your helper expects
  temperature: 0.0
  max_tokens: 256


experiment:
  name: "baseline_vs_cot_fewshot"
  methods: ["baseline", "cot", "fewshot"]
  dataset: "file"
  dataset_path: "data/tasks_v1.json"
  output_dir: "results"

fewshot_examples:
  - { input: "I loved the dinner, it was great.", output: "positive" }
  - { input: "I hated the movie, it was terrible.", output: "negative" }


